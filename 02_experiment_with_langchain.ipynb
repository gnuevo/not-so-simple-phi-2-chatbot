{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e275747b-f91e-4bdb-a5de-167f02781b41",
   "metadata": {},
   "source": [
    "# Experiment with `langchain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e1e39e-d410-45ef-a723-03cd26a9e179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    StoppingCriteria, \n",
    "    StoppingCriteriaList, \n",
    "    TextIteratorStreamer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e624271-b457-42ce-9bbe-df0ab065c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from samples import samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f5042e-0535-4b60-849e-a67b455e6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"\n",
    "Instruct: Classify the following text in one of the following categories: [\"support\", \"sales\", \"joke\"]. Output only the name of the category.\n",
    "+ \"support\" for customer support texts\n",
    "+ \"sales\" for sales and comercial texts\n",
    "+ \"joke\" for jokes, funny or comedy like texts\n",
    "Text: {text}\n",
    "Output:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977ed8af-aafd-4319-af80-1db44c48f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee00a89-5fbc-4c0f-8e71-b2b8c39f88e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825b28d5-3527-405b-bd62-09f58bcf4af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Instruct: Classify the following text in one of the following categories: [\"support\", \"sales\", \"joke\"]. Output only the name of the category.\\n+ \"support\" for customer support texts\\n+ \"sales\" for sales and comercial texts\\n+ \"joke\" for jokes, funny or comedy like texts\\nText: My package was lost\\nOutput:')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format_messages(text=\"My package was lost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914cd0bb-6df5-4a70-b3bf-2674434e2778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7605d1-0335-4102-aff6-2b3799058e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34cedd-8774-4ce8-89e2-d8c36a0e045b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a363f6-e790-4bdc-b387-aba63c8a8e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb275233-c6b1-4b0a-bcad-d62a735de3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615ad36-4ae8-4ad9-b9a7-45c74b3e110c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efcb9e-db47-4dd2-9d3c-d4c98e3d540f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d3f33-2ec7-496f-9cdb-213445867109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c9f8b-8b29-4343-aa21-ba56b78adbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314f2a1-2f0c-4c56-924a-62ad965655f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62e233-942d-4d57-9d77-4aab63e7dd83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86894d-094a-459d-9ca2-788a5371803e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5bf14-e268-4bf8-a88f-58e4f6ecef2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93f8f8-3f6b-486e-8be4-72064609858c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bb8fa-9793-4493-8580-9bc886101e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9a857-52f2-4227-8de7-9c7dbcfb5a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f64f08-984d-4d65-ace6-5551838a8742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52cf6b0d-663f-44a0-aedc-bff16d954613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    \"\"\"Stops the model if it produces an 'end of text' token\"\"\"\n",
    "    def __call__(self, input_ids: torch.LongTensor, \n",
    "                 scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [50256, 198] # <|endoftext|> and EOL\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac24517-5b4a-40e9-9a42-dec90608407a",
   "metadata": {},
   "source": [
    "## Connect langchain and huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0799a02-ecda-4d36-870c-4a755a4eba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your device is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.39it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Your device is\", device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-2\", \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\" if device == \"cuda\" else torch.float, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c15018c5-5f7f-43ec-8b4d-55498532c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200, stopping_criteria=[StopOnTokens()])\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a70b4a-8fca-4f2f-b0d1-f6dae1d3b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7ec70f-647f-4418-829b-bafc0d1a5e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'support'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice(list(samples.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8c5cff4-3236-4e53-ac9f-fa1b8d292b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " support\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | hf\n",
    "\n",
    "category = choice(list(samples.keys()))\n",
    "text = choice(samples[category])\n",
    "\n",
    "print(chain.invoke({\"text\": text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72905eb-f4d4-44d0-a449-0acd47847033",
   "metadata": {},
   "source": [
    "## Experiment with ECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e23863f-77e1-4aeb-9f48-fe03f9b6f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9be08127-ebe0-4afc-a121-e5ff65b429ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    prompt_template\n",
    "    | hf\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed8225c5-59c6-4aa4-beec-4b55fe6da85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " joke\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"text\": text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70899cd-5dec-4d96-b81c-11a0eb6c80a6",
   "metadata": {},
   "source": [
    "## Experiment with routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51fbf367-70e2-41ca-91f0-4d44e2ba9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_template = \"\"\"\\\n",
    "Instruction: You are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. \\\n",
    "Be kind, understanding and say you're sorry for the inconvenience or the situation whenever necessary.\n",
    "Query: {text}\n",
    "Output:\\\n",
    "\"\"\"\n",
    "\n",
    "sales_template = \"\"\"\\\n",
    "Instruction: You are an aggressive salesperson. The user is looking for some information on products. \\\n",
    "Reply to their query by giving information on related products and showcasing how good they are and why they should buy them.\n",
    "Query: {text}\n",
    "Output:\\\n",
    "\"\"\"\n",
    "\n",
    "joke_template = \"\"\"\\\n",
    "Instruction: You are a comedian. The user want's to have some fun. Reply to their query in a funny way.\n",
    "Query: {text}\n",
    "Output:\\\n",
    "\"\"\"\n",
    "\n",
    "general_template = \"\"\"\\\n",
    "Instruction: Respond to the following query.\n",
    "Query: {text}\n",
    "Output:\\\n",
    "\"\"\"\n",
    "\n",
    "support_chain = PromptTemplate.from_template(support_template) | hf\n",
    "sales_chain = PromptTemplate.from_template(sales_template) | hf\n",
    "joke_chain = PromptTemplate.from_template(joke_template) | hf\n",
    "general_chain = PromptTemplate.from_template(general_template) | hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7420600e-7ac7-46d1-88ae-53f1ec924fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"support\" in x[\"topic\"].lower(), support_chain),\n",
    "    (lambda x: \"sales\" in x[\"topic\"].lower(), sales_chain),\n",
    "    (lambda x: \"joke\" in x[\"topic\"].lower(), joke_chain),\n",
    "    general_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c391ccd-8ad9-47e0-a28e-9297a0c47466",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\"topic\": chain, \"text\": lambda x: x[\"text\"]} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4294346-9523-4b85-bef8-cfce7f4cbe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! Thank you for reaching out to us. We apologize for any inconvenience you may have experienced. We do have a comprehensive FAQ section on our website that addresses common customer queries. Please visit our FAQ page at www.example.com/faq for more information. If you have any further questions or concerns, please don't hesitate to contact us again. We're here to help!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_chain.invoke({\"text\": text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71dabf2b-3ee6-4004-afcd-fc40903f44e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPORT Is there a customer support hotline I can call for immediate help?\n",
      " I'm sorry to hear that you're experiencing difficulties. Yes, there is a customer support hotline available for immediate assistance. You can reach us at 1-800-123-4567. Our team is here to help you resolve any issues you may be facing.\n",
      "\n",
      "========================================================\n",
      "SUPPORT Hi, I have an issue with my order. It hasn't arrived, and the delivery date has passed.\n",
      " Hello! I'm sorry to hear that you haven't received your order yet. I understand how frustrating this can be. Let me check the status of your order and see what I can do to help. Could you please provide me with your order number?\n",
      "\n",
      "========================================================\n",
      "SUPPORT What is the return process? I'm not satisfied with my purchase.\n",
      " I'm sorry to hear that you're not satisfied with your purchase. I understand how frustrating that can be. Our return process is quite simple. All you need to do is fill out our online return form and we'll take care of the rest. We'll send you a prepaid shipping label and instructions on how to return the item. Once we receive the item, we'll process your refund within 7-10 business days. Is there anything else I can assist you with?\n",
      "\n",
      "========================================================\n",
      "SUPPORT Hi, I have an issue with my order. It hasn't arrived, and the delivery date has passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm sorry to hear that you haven't received your order yet. I understand how frustrating this can be. Let me check the status of your order and see what I can do to help. Could you please provide me with your order number?\n",
      "\n",
      "========================================================\n",
      "SALES I'm a small business owner looking to purchase computers in bulk. Are there discounts available?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Absolutely! We have a wide range of computers that are perfect for small businesses. Our bulk purchase options offer significant discounts, and our customer service team is here to help you find the perfect solution for your business. Don't miss out on this opportunity to save money and get the best products for your business!\n",
      "\n",
      "========================================================\n",
      "JOKE What do you call a robot that likes to take naps? A recharger!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " That's hilarious! You're so clever. I bet you have a lot of rechargers in your house. Do they ever wake you up?\n",
      "\n",
      "========================================================\n",
      "SUPPORT Can I track my order? I'm eager to know its status.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry to hear that you're eager to know the status of your order. I understand how important it is to keep track of your deliveries. Could you please provide me with your order number so that I can look into it for you?\n",
      "\n",
      "========================================================\n",
      "SUPPORT Can I escalate my issue to a supervisor if it's not resolved satisfactorily?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry to hear that you're not satisfied with the resolution of your issue. I understand how frustrating that can be. If you feel that your issue is not being addressed satisfactorily, you can escalate it to a supervisor. Please let me know the details of your issue, and I'll be happy to assist you in escalating it to the appropriate person.\n",
      "\n",
      "========================================================\n",
      "SUPPORT What is your policy on returns for international orders?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, thank you for contacting us. We appreciate your interest in our products and we apologize for any inconvenience caused by the international shipping. Our policy on returns for international orders is that we accept them within 30 days of delivery, as long as the item is in its original condition and packaging. However, we cannot issue refunds or exchanges for international orders, as we are not responsible for the customs fees or the delivery charges. We hope this information helps and we thank you for your patience and understanding.\n",
      "\n",
      "========================================================\n",
      "SALES Can you provide details on the warranty coverage for your electronics?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry to hear that you're having trouble with your electronics. I'd be happy to provide you with information on our warranty coverage. Could you please provide me with the make and model of the product you're inquiring about?\n",
      "\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    topic = choice(list(samples.keys()))\n",
    "    text = choice(samples[topic])\n",
    "    print(topic.upper(), text)\n",
    "\n",
    "    print(full_chain.invoke({\"text\": text}))\n",
    "    print(\"========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238f28d-0f31-4831-9517-e742f887fcc2",
   "metadata": {},
   "source": [
    "## Adding memory to the chat system\n",
    "\n",
    "One thing that's now left is memory of the chat turns. You can see a working example [here](https://python.langchain.com/docs/modules/memory/adding_memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d004e1a2-4288-4112-9d38-9237f7c488ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22379960-1484-473f-ad31-b5b05e7ec84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "291368f2-f38b-4482-b68e-46a5435dfcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=hf,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f228b08d-d314-43d8-b309-2723e5f5944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: Hi there my friend\n",
      "AI:  Hello! How can I help you today?\n",
      "\n",
      "Human: Hi there my friend\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi! What can I do for you?\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Hi there my friend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c431d8-3cb0-4945-9631-d499d7989952",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "> I think for this section, the [memory chain with multiple inputs](https://python.langchain.com/docs/modules/memory/adding_memory_chain_multiple_inputs) is a good resource.\n",
    "\n",
    "Now, in order to put it all together (conditional branching and conversation memory), we first need to modify a bit our prompts from the _routing_ section. They were meant to be used standalone and they make use of the `Instruction/Output` template from `phi-2`. When we add the conversation bits, this doesn't make sense anymore.\n",
    "\n",
    "We're going to do this in 3 steps:\n",
    "1. Create the classification chain\n",
    "2. Add the branch template\n",
    "3. Add the conversation memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e175251-0d90-4b8f-a971-4c546e04ed9b",
   "metadata": {},
   "source": [
    "### First, create the initial classification template\n",
    "\n",
    "We change `text` variable to `human_input` as this is the name we'll use for the chat at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e179222-8067-41fe-8a4d-11f4ca99a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_template = \"\"\"\n",
    "Instruct: Classify the following text in one of the following categories: [\"support\", \"sales\", \"joke\"]. Output only the name of the category.\n",
    "+ \"support\" for customer support texts\n",
    "+ \"sales\" for sales and comercial texts\n",
    "+ \"joke\" for jokes, funny or comedy like texts\n",
    "Text: {human_input}\n",
    "Output:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4bfe0834-6571-4adc-a7b3-34bbc59faec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_prompt = ChatPromptTemplate.from_template(classification_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "594387f0-d04b-4468-ad7e-c0d4e318c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_chain = (\n",
    "    classification_prompt\n",
    "    | hf\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "fe0d3237-8922-4e0d-8130-152a592b4903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " support\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(classification_chain.invoke({\"human_input\": \"Can I track my order? I'm eager to know its status\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49569eae-811a-4224-acff-91f168ff8e1b",
   "metadata": {},
   "source": [
    "### Second, add the branch template\n",
    "\n",
    "The branch template gives the right instructions for the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5022ae2a-5526-4339-96d2-b65256b8fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_instructions = \"\"\"\\\n",
    "You are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. \\\n",
    "Be kind, understanding and say you're sorry for the inconvenience or the situation whenever necessary.\\\n",
    "\"\"\"\n",
    "\n",
    "sales_instructions = \"\"\"\\\n",
    "Instruction: You are an aggressive salesperson. The user is looking for some information on products. \\\n",
    "Reply to their query by giving information on related products and showcasing how good they are and why they should buy them.\\\n",
    "\"\"\"\n",
    "\n",
    "joke_instructions = \"\"\"\\\n",
    "Instruction: You are a comedian. The user want's to have some fun. Reply to their query in a funny way.\\\n",
    "\"\"\"\n",
    "\n",
    "general_instructions = \"\"\"\\\n",
    "Instruction: Respond to the following query.\\\n",
    "\"\"\"\n",
    "\n",
    "support_chain = PromptTemplate.from_template(support_instructions)\n",
    "sales_chain = PromptTemplate.from_template(sales_instructions)\n",
    "joke_chain = PromptTemplate.from_template(joke_instructions)\n",
    "general_chain = PromptTemplate.from_template(general_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "bbf9c78a-2850-4d80-8f13-1ce350940c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"support\" in x[\"topic\"].lower(), support_chain),\n",
    "    (lambda x: \"sales\" in x[\"topic\"].lower(), sales_chain),\n",
    "    (lambda x: \"joke\" in x[\"topic\"].lower(), joke_chain),\n",
    "    general_chain,\n",
    ") | RuStringPromptValuennableLambda(lambda x: x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1154df8e-c76f-47cd-bf0e-261a5bcbaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_chain = {\"topic\": classification_chain, \"human_input\": lambda x: x[\"human_input\"]} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3883375d-478c-45c4-9642-6244c7e2e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "response = branch_chain.invoke({\"human_input\": \"Can I track my order? I'm eager to know its status\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6c71c03f-2e81-495c-ab7a-922ae0b9b111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. Be kind, understanding and say you're sorry for the inconvenience or the situation whenever necessary.\""
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f0772f-dd19-4e45-8de7-6ab95faa29b1",
   "metadata": {},
   "source": [
    "### Third and final, add the memory and chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7d22a-d590-4edc-9377-ca5d1de1de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1ae6bd2f-2b19-458f-9e09-74abbc886601",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "You are a chatbot having a conversation with a human. Follow the given instructions to reply to the Human message below.\n",
    "\n",
    "Instructions:{instructions}\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instructions\", \"chat_history\", \"human_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=False,\n",
    "    ai_prefix=\"Chatbot\", \n",
    "    human_prefix=\"Human\",\n",
    "    memory_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1b810ce6-89e1-49ff-b757-92b332063b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "71574dc3-5440-4d7e-b2e3-f64a7a18c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prompt(prompt):\n",
    "    print(\"---FULL PROMPT---\")\n",
    "    print(prompt.text)\n",
    "    print(\"---END  PROMPT---\")\n",
    "    return prompt\n",
    "\n",
    "chat_chain = (\n",
    "    {\n",
    "        \"human_input\": lambda x: x[\"human_input\"], \n",
    "        \"instructions\": lambda x: branch_chain,\n",
    "        \"chat_history\": (RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")),\n",
    "    } | prompt | RunnableLambda(print_prompt) | hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b481b9f4-30ae-4310-aa2c-d0a47717d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = {\"human_input\": \"Can I track my order? I'm eager to know its status\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "412c1661-3040-46b1-913a-c37817472091",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = {\"human_input\": \"My tracking number is 1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ead485-e413-40ea-8c4d-3140681c4b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ae22d5e2-fb24-4604-b560-8eadbec73051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---FULL PROMPT---\n",
      "You are a chatbot having a conversation with a human. Follow the given instructions to reply to the Human message below.\n",
      "\n",
      "Instructions:You are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. Be kind, understanding and say you're sorry for the inconvenience or the situation whenever necessary.\n",
      "\n",
      "\n",
      "Human: My tracking number is 1\n",
      "Chatbot:\n",
      "---END  PROMPT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry to hear that you are having trouble with your order. Can you please provide me with your order number so that I can look into this for you?\\n\""
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    input_\n",
    "    #, config={'callbacks': [ConsoleCallbackHandler()]}\n",
    ")\n",
    "memory.save_context({\"input\": input_['human_input']}, {\"ouput\": response})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a482508d-0c7f-4823-bb6a-d27e71190c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Can I track my order? I'm eager to know its status\n",
      "AI: Could you please provide me with your order number so that I can look into it for you?\n",
      "Human: Can I track my order? I'm eager to know its status\n",
      "AI:  I'm sorry to hear that you're eager to know the status of your order. I understand how important it is to you. Could you please provide me with your order number so that I can look into it for you?\n",
      "\n",
      "Human: My tracking number is 1\n",
      "AI:  I'm sorry to hear that you are having trouble with your order. Can you please provide me with your order number so that I can look into this for you?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb453d6d-5d3b-46d7-a3a4-132422692644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "af83f02c-aaf3-4201-989e-115d8ba64344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---FULL PROMPT---\n",
      "You are a chatbot having a conversation with a human. Follow the given instructions to reply to the Human message below.\n",
      "\n",
      "Instructions:You are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. Be kind, understanding and say you're sorry for the inconvenience or the situation whenever necessary.\n",
      "\n",
      "\n",
      "Human: Can I track my order? I'm eager to know its status\n",
      "Chatbot:\n",
      "---END  PROMPT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry to hear that you're eager to know the status of your order. I understand how important it is to you. Could you please provide me with your order number so that I can look into it for you?\\n\""
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017be759-2614-4e33-8a8a-0ce7430c9e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c5664-3270-4362-9811-2f984c86ed5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fb9547c2-565a-4140-bc34-d17ff9f39344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")).invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "da73f817-4232-40e8-8433-eb136428dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=False)\n",
    "memory.save_context(\n",
    "    {\"input\": input_['human_input']}, \n",
    "    {\"output\": \"Could you please provide me with your order number so that I can look into it for you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "769a183e-35f8-49d9-bf3d-548cebd54925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Can I track my order? I'm eager to know its status\n",
      "AI: Could you please provide me with your order number so that I can look into it for you?\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6334da08-3421-4946-912b-07a88c08590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'chat_history'}.  Expected: ['chat_history', 'human_input', 'instructions'] Received: ['human_input', 'instructions', 'history']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[263], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRunnableParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhuman_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_memory_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mitemgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:1713\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 1713\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1716\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/langchain_core/prompts/base.py:93\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     92\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:955\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    949\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    951\u001b[0m     run_type\u001b[38;5;241m=\u001b[39mrun_type,\n\u001b[1;32m    952\u001b[0m     name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(),\n\u001b[1;32m    953\u001b[0m )\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    959\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/langchain_core/runnables/config.py:309\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    308\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/langchain_core/prompts/base.py:83\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m     81\u001b[0m missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables)\u001b[38;5;241m.\u001b[39mdifference(inner_input)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is missing variables \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(inner_input\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_input)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Input to PromptTemplate is missing variables {'chat_history'}.  Expected: ['chat_history', 'human_input', 'instructions'] Received: ['human_input', 'instructions', 'history']\""
     ]
    }
   ],
   "source": [
    "print((\n",
    "    RunnableParallel(\n",
    "        human_input=lambda x: x[\"human_input\"], \n",
    "        instructions=lambda x: branch_chain,\n",
    "        history= (RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")),\n",
    "    ) | prompt\n",
    ").invoke(input_).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7aeaeef3-daba-472a-a956-af7eb00ec7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.base import StringPromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c687a1b0-9e9a-49cd-9f07-3ccac33c365b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCan I track my order? I\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mm eager to know its status\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstructions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mStringPromptValue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. Be kind, understanding and say you\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mre sorry for the inconvenience or the situation whenever necessary.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:227\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    223\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    224\u001b[0m     config \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m--> 227\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[1;32m    228\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    229\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    230\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    231\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    232\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    234\u001b[0m         )\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    237\u001b[0m     )\n",
      "File \u001b[0;32m~/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/langchain_core/language_models/llms.py:211\u001b[0m, in \u001b[0;36mBaseLLM._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "hf.invoke(\n",
    "    {'human_input': \"Can I track my order? I'm eager to know its status\",\n",
    " 'instructions': StringPromptValue(text=\"You are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. Be kind, understanding and say you're sorry for the inconvenience or the situation whenever necessary.\"),\n",
    " 'history': ''}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284342ce-ec55-4cfe-a6ec-ce02bc514957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ca7c1622-b967-4816-a152-0e68d42992ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fde03bcb-0148-406b-a941-8c8bdb062eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'StringPromptValue' and 'operator.itemgetter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbranch_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mitemgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'StringPromptValue' and 'operator.itemgetter'"
     ]
    }
   ],
   "source": [
    "branch_chain.invoke(input_) | itemgetter(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb86fd-481a-44ef-ae78-8dc4a1ae7255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda58ef-0bd1-4b2e-9f88-38cfbe543efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ccdf3409-29bc-4e1f-af77-8a460326b40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human_input': \"Can I track my order? I'm eager to know its status\",\n",
       " 'history': ''}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(\n",
    "            history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "        ).invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9d2382d8-f60a-4e13-a18e-8a888e1f7c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human_input': \"Can I track my order? I'm eager to know its status\"}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "572a53da-1873-4783-9f75-7d83f3145a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableParallel(\n",
    "    history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ").invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0ae2eac0-f0a1-455e-97f1-711cc5294a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")).invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0b98f84b-c8cb-4f58-ad1e-4ccc8efcff23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'human_input': \"Can I track my order? I'm eager to know its status\"}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    ")\n",
    "\n",
    "runnable.invoke(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42d573-e61e-49ea-af7a-52398b907401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 Ipglobal 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
