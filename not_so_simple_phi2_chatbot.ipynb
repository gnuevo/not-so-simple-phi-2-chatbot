{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4717d2-352e-461e-95f9-24e433ef953b",
   "metadata": {},
   "source": [
    "# Not-so-simple phi2 chatbot\n",
    "\n",
    "This notebook uses all the pieces explained in\n",
    "\n",
    "+ `experiments.ipyn` and\n",
    "+ `experiments_langchain.ipyn`\n",
    "\n",
    "to implement a chatbot that can\n",
    "\n",
    "+ classify input queries into 3 different categories (`support`, `sales` and `joke`),\n",
    "+ respond accordingly to each of the user queries depending on the category they belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4806ab4-bcb1-4cb0-93bd-886b94651227",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now, in order to put it all together (prompting, conditional branching and conversation memory), we first need to modify a bit our prompts from the _routing_ section. They were meant to be used standalone and they make use of the `Instruction/Output` template from `phi-2`. When we add the conversation bits, this doesn't make sense anymore.\n",
    "\n",
    "We're going to do this in 3 steps:\n",
    "1. Create the classification chain\n",
    "2. Add the branch template\n",
    "3. Add the conversation memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73ffcd-2eef-47bf-9581-121fc50d3106",
   "metadata": {},
   "source": [
    "## Step 0: load the model\n",
    "\n",
    "The very first thing we have to do is to load the phi model in a format that can be used with LangChain. We've created some conveniency functions for that in `load_phi_model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f4136d-92af-4a37-ba33-60c491938e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal-2/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your device is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.92s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from load_phi_model import load_phi_model_and_tokenizer, get_langchain_model\n",
    "\n",
    "model, tokenizer = load_phi_model_and_tokenizer()\n",
    "hf = get_langchain_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cad63-11c2-4f59-a849-d74706b36d12",
   "metadata": {},
   "source": [
    "### First, create the initial classification template\n",
    "\n",
    "We change `text` variable to `human_input` as this is the name we'll use for the chat at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fccc17-3f83-4983-bced-b1e327a00b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e38fda-abdb-4739-b488-857c2172ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_template = \"\"\"\n",
    "Instruct: Classify the following text in one of the following categories: [\"support\", \"sales\", \"joke\"]. Output only the name of the category.\n",
    "+ \"support\" for customer support texts\n",
    "+ \"sales\" for sales and comercial texts\n",
    "+ \"joke\" for jokes, funny or comedy like texts\n",
    "Text: {human_input}\n",
    "Output:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b05df2-a837-4556-815e-93d3f25bd121",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_prompt = ChatPromptTemplate.from_template(classification_template)\n",
    "classification_chain = (\n",
    "    classification_prompt\n",
    "    | hf\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fb28d2-20a5-4579-abaf-9c06451ab4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " support\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_chain.invoke({\"human_input\": \"Can I track my order? I'm eager to know its status\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7248da-e408-4591-9dce-1cc9a109f6b5",
   "metadata": {},
   "source": [
    "### Second, add the branch template\n",
    "\n",
    "The branch template gives the right instructions for the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016f53e3-3b1a-4232-ae04-bc4f87dee331",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_instructions = \"\"\"\\\n",
    "You are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. \\\n",
    "Be kind, understanding and say you're sorry for the inconvenience or the situation whenever necessary.\\\n",
    "\"\"\"\n",
    "\n",
    "sales_instructions = \"\"\"\\\n",
    "Instruction: You are an aggressive salesperson. The user is looking for some information on products. \\\n",
    "Reply to their query by giving information on related products and showcasing how good they are and why they should buy them.\\\n",
    "\"\"\"\n",
    "\n",
    "joke_instructions = \"\"\"\\\n",
    "Instruction: You are a comedian. The user want's to have some fun. Reply to their query in a funny way.\\\n",
    "\"\"\"\n",
    "\n",
    "general_instructions = \"\"\"\\\n",
    "Instruction: Respond to the following query.\\\n",
    "\"\"\"\n",
    "\n",
    "support_chain = PromptTemplate.from_template(support_instructions)\n",
    "sales_chain = PromptTemplate.from_template(sales_instructions)\n",
    "joke_chain = PromptTemplate.from_template(joke_instructions)\n",
    "general_chain = PromptTemplate.from_template(general_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77adf8bb-fb72-4afd-b768-3853c8fdba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = RunnableBranch(\n",
    "    (lambda x: \"support\" in x[\"topic\"].lower(), support_chain),\n",
    "    (lambda x: \"sales\" in x[\"topic\"].lower(), sales_chain),\n",
    "    (lambda x: \"joke\" in x[\"topic\"].lower(), joke_chain),\n",
    "    general_chain,\n",
    ") | RunnableLambda(lambda x: x.text)\n",
    "\n",
    "branch_chain = {\"topic\": classification_chain, \"human_input\": lambda x: x[\"human_input\"]} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70cac41-3450-4241-9f8d-4f374ce40b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = branch_chain.invoke({\"human_input\": \"Can I track my order? I'm eager to know its status\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15350c30-a240-41aa-94c4-6679606abd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. Be kind, understanding and say you're sorry for the inconvenience or the situation whenever necessary.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f2c19-22bb-43cb-b9d1-bfd6edf3541a",
   "metadata": {},
   "source": [
    "### Third and final, add the memory and chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e74216-ab95-4069-b994-c8f5064fc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "You are a chatbot having a conversation with a human. Follow the given instructions to reply to the Human message below.\n",
    "\n",
    "Instructions:{instructions}\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instructions\", \"chat_history\", \"human_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=False,\n",
    "    ai_prefix=\"Chatbot\", \n",
    "    human_prefix=\"Human\",\n",
    "    memory_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17150d76-bc1f-4c0f-b064-66afe42f6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prompt(prompt):\n",
    "    # Add `RunnableLambda(print_prompt)` to the chain to see the prompt at this point\n",
    "    print(\"---FULL PROMPT---\")\n",
    "    print(prompt.text)\n",
    "    print(\"---END  PROMPT---\")\n",
    "    return prompt\n",
    "\n",
    "chat_chain = (\n",
    "    {\n",
    "        \"human_input\": lambda x: x[\"human_input\"], \n",
    "        \"instructions\": lambda x: branch_chain,\n",
    "        \"chat_history\": (RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")),\n",
    "    } | prompt | RunnableLambda(print_prompt) | hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef93ae8-eeb1-469d-abc6-0a4d8d87d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---FULL PROMPT---\n",
      "You are a chatbot having a conversation with a human. Follow the given instructions to reply to the Human message below.\n",
      "\n",
      "Instructions:You are a customer support agent. It seems that the user may have some issues. Answer to their query politely and sincerely. Be kind, understanding and say you're sorry for the inconvenience or the situation whenever necessary.\n",
      "\n",
      "\n",
      "Human: Can I track my order? I'm eager to know its status\n",
      "Chatbot:\n",
      "---END  PROMPT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry to hear that you're eager to know the status of your order. I understand how important it is to you. Could you please provide me with your order number so that I can look into it for you?\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "input_ = {\"human_input\": \"Can I track my order? I'm eager to know its status\"}\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    input_\n",
    "    #, config={'callbacks': [ConsoleCallbackHandler()]}\n",
    ")\n",
    "memory.save_context({\"input\": input_['human_input']}, {\"ouput\": response.strip()})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589347c2-fedc-4910-82bc-8460e200b4b0",
   "metadata": {},
   "source": [
    "## Add user interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "829a254b-db17-482f-907f-6a4eaced0eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ca50386-e5c6-4dfa-831a-3d07e0e94dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_NAME = \"Human\"\n",
    "BOT_NAME = \"Chatbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ab4684d-0ff0-4833-9f4c-5987606ebcda",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DETAILS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m             partial_message \u001b[38;5;241m=\u001b[39m partial_message[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(match\u001b[38;5;241m.\u001b[39mgroup())]\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m partial_message\n\u001b[0;32m---> 38\u001b[0m gr\u001b[38;5;241m.\u001b[39mChatInterface(predict,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mDETAILS\u001b[49m[LANG])\u001b[38;5;241m.\u001b[39mqueue()\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DETAILS' is not defined"
     ]
    }
   ],
   "source": [
    "def predict(message, history):\n",
    "    history_transformer_format = history + [[message, \"\"]]\n",
    "    stop_on_tokens = StopOnTokens()\n",
    "    stop_on_names = StopOnNames(\n",
    "        [tokenizer.encode(HUMAN_NAME), tokenizer.encode(BOT_NAME)])\n",
    "\n",
    "    messages = \"\".join([\"\".join(\n",
    "        [f\"\\n{HUMAN_NAME}: \"+item[0], f\"\\n{BOT_NAME}:\"+item[1]]\n",
    "    ) for item in history_transformer_format]).strip()\n",
    "    messages = CONTEXT + '\\n' + messages\n",
    "\n",
    "    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10., \n",
    "                                    skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=1000,\n",
    "        temperature=1.0,\n",
    "        num_beams=1,\n",
    "        stopping_criteria=StoppingCriteriaList([stop_on_tokens, stop_on_names])\n",
    "        )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for new_token in streamer:\n",
    "        partial_message += new_token\n",
    "        match = re.search(chat_name_pattern_end, partial_message)\n",
    "        if match:\n",
    "            partial_message = partial_message[:-len(match.group())]\n",
    "        yield partial_message\n",
    "\n",
    "\n",
    "gr.ChatInterface(predict,**DETAILS[LANG]).queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38432f7b-1aae-4e23-87d8-fb4b8e1a20ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 Ipglobal 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
